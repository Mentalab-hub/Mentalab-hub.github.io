<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Applications on Wiki</title>
    <link>http://localhost:1313/applications/</link>
    <description>Recent content in Applications on Wiki</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 10:00:00 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/applications/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Motor Imagery BCI</title>
      <link>http://localhost:1313/applications/motor-imagery-bci/</link>
      <pubDate>Mon, 24 Mar 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/motor-imagery-bci/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;This application is a Brain-Computer Interface (BCI) system designed to classify motor imagery tasks (left hand, right hand, and neutral state) using EEG data. The system connects to Explore Pro device via the Lab Streaming Layer (LSL) protocol, processes the EEG signals, and classifies the data using machine learning models, including a Transformer-based neural network.&lt;/p&gt;&#xA;&lt;p&gt;The application supports real-time classification. It includes tools for data collection, preprocessing, model training, and evaluation. The code for the application can be found in our &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy/tree/master/examples&#34;&gt;explorepy/examples section on github&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artefact removal</title>
      <link>http://localhost:1313/applications/artefact-removal/</link>
      <pubDate>Tue, 25 Mar 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/artefact-removal/</guid>
      <description>&lt;p&gt;This is an implementation of the &lt;a href=&#34;https://www.plasticityinneurodevelopmentlab.com/happilee-lowdensity-eeg&#34;&gt;HAPPILEE pipeline&lt;/a&gt; for the &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy&#34;&gt;Explore system&lt;/a&gt; from Mentalab. See our &lt;a href=&#34;https://github.com/Mentalab-hub/explore-happilee.git&#34;&gt;github page&lt;/a&gt; for the code for this project.&lt;/p&gt;&#xA;&lt;p&gt;The HAPPILEE pipeline is applied to EEG data recorded by the Mentalab Explore Pro device in &lt;code&gt;happilee.py&lt;/code&gt; and &lt;code&gt;happilee.ipynb&lt;/code&gt;. &lt;code&gt;happilee.py&lt;/code&gt; is the main script to apply HAPPILEE to an EXG dataset recorded by the Explore system. &lt;code&gt;happilee.ipynb&lt;/code&gt; is a notebook to demonstrate and track steps of the HAPPILEE pipeline to a dataset recorded by the Explore system.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sleep Actigraphy</title>
      <link>http://localhost:1313/applications/sleep-actigraphy/</link>
      <pubDate>Thu, 12 Jun 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/sleep-actigraphy/</guid>
      <description>&lt;h1 id=&#34;sleep-activity-pipeline-with-pyactigraphy-accelerometer-and-yasa&#34;&gt;Sleep Activity Pipeline (with PyActigraphy, Accelerometer and YASA)&lt;/h1&gt;&#xA;&lt;p&gt;Here&amp;rsquo;s a more polished version of your text, with fewer bullet points and a smoother flow:&lt;/p&gt;&#xA;&lt;p&gt;Sleep Activity Analysis with PyActigraphy, Accelerometer, and YASA&#xA;This guide demonstrates a comprehensive pipeline for analyzing sleep stages and activity patterns using EEG and accelerometer data acquired from a Mentalab Explore device. For those specifically interested in sleep staging, our &lt;a href=&#34;https://wiki.mentalab.com/applications/sleep-analysis/&#34;&gt;Sleep EEG Analysis&lt;/a&gt; application example offers further insights.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Photodiode Event Triggers</title>
      <link>http://localhost:1313/applications/photodiode-event-trigger/</link>
      <pubDate>Wed, 26 Jan 2022 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/photodiode-event-trigger/</guid>
      <description>&lt;p&gt;Synchronizing ExG signals with stimuli is critical to many research paradigms. There are two main approaches to synchronizing data: software event markers and hardware triggers.&lt;/p&gt;&#xA;&lt;p&gt;Although software event markers are easy to use, they are not as precise as hardware triggers. This is because the data is sent via Bluetooth, which has some inherent delays.&lt;/p&gt;&#xA;&lt;p&gt;Meanwhile, hardware triggers are much more precise but involve connecting a wire between devices (e.g. a computer and an Explore device). This means your Mentalab Explore system is no longer fully wireless (although presenting stimuli on a mobile can mitigate this).&lt;/p&gt;</description>
    </item>
    <item>
      <title>EOG Artifact Removal</title>
      <link>http://localhost:1313/applications/power-bands/</link>
      <pubDate>Wed, 26 Jan 2022 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/power-bands/</guid>
      <description>&lt;p&gt;Many EEG applications analyse EEG signals for frequencies that are associated with specific activities.&lt;/p&gt;&#xA;&lt;p&gt;Here, is an &lt;a href=&#34;http://openvibe.inria.fr/&#34;&gt;OpenViBE&lt;/a&gt; pipeline that breaks down raw EEG signals into frequency bands and removes blinking artifacts in real-time.&lt;/p&gt;&#xA;&lt;h2 id=&#34;set-up&#34;&gt;Set-up&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Explorepy using &lt;a href=&#34;https://explorepy.readthedocs.io/en/latest/installation.html&#34;&gt;Explorepy installation guide&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Install Explore Desktop from &lt;a href=&#34;https://github.com/Mentalab-hub/explore-desktop-release/releases/latest&#34;&gt;Explore Desktop Github page&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Prepare an 8-channel &lt;a href=&#34;https://mentalab.com/products/&#34;&gt;Mentalab Explore system&lt;/a&gt; using the following configuration. You can use wet or dry electrodes for this, and modify as needed.&lt;br&gt;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;  Explore Channel&lt;/th&gt;&#xA;          &lt;th&gt;  Cap Location&lt;/th&gt;&#xA;          &lt;th&gt;  OpenViBE Channel&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch1&lt;/td&gt;&#xA;          &lt;td&gt;F3&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch2&lt;/td&gt;&#xA;          &lt;td&gt;C3&lt;/td&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch3&lt;/td&gt;&#xA;          &lt;td&gt;Forehead (mid)&lt;/td&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch4&lt;/td&gt;&#xA;          &lt;td&gt;C4&lt;/td&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch7&lt;/td&gt;&#xA;          &lt;td&gt;Right eye (beside outer canthus)&lt;/td&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Ch8&lt;/td&gt;&#xA;          &lt;td&gt;F4&lt;/td&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Check impedances using &lt;a href=&#34;../../explore-desktop-guide/&#34;&gt;Explore Desktop&lt;/a&gt;. Aim for &amp;lt;20 kΩ for wet electrodes, and &amp;lt;150 kΩ for dry electrodes. For more on this function, please watch our &lt;a href=&#34;https://youtu.be/Q237Ny20SDI?t=152&amp;amp;si=g-e5j_K3likP7-Xo&#34;&gt;video guide&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;To visualize your data, consider using &lt;a href=&#34;../../explore-desktop-guide/&#34;&gt;Explore Desktop&lt;/a&gt;. By visualizing your data, you can ensure there is no obvious noise. &lt;a href=&#34;https://youtu.be/Q237Ny20SDI?t=213&amp;amp;si=uyRLScOAreZfL1jw&#34;&gt;video guide&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Finally, push your data stream to LSL using the &lt;code&gt;&lt;a href=&#34;https://explorepy.readthedocs.io/en/latest/usage.html#push2lsl&#34;&gt;explorepy push2lsl&lt;/a&gt;&lt;/code&gt; command. In Explore Desktop, simply click the &lt;code&gt;LSL&lt;/code&gt; button in the main visualization menu.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;run-the-script&#34;&gt;Run the script&lt;/h2&gt;&#xA;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;Download this &lt;a href=&#34;http://localhost:1313/files/Mentalab-EEG_Power_Bands.zip&#34;&gt;ZIP folder&lt;/a&gt; and extract the contents.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://openvibe.inria.fr/downloads/&#34;&gt;Install OpenViBE&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;start-streaming&#34;&gt;Start streaming&lt;/h3&gt;&#xA;&lt;ol start=&#34;9&#34;&gt;&#xA;&lt;li&gt;Open an OpenViBE &lt;code&gt;acquisition server&lt;/code&gt; and select &lt;code&gt;LabStreamingLayer&lt;/code&gt; as the driver.&lt;/li&gt;&#xA;&lt;li&gt;Under &lt;code&gt;Driver Properties&lt;/code&gt; select Explore&amp;rsquo;s &lt;code&gt;ExG&lt;/code&gt; and &lt;code&gt;Marker&lt;/code&gt; streams.&lt;/li&gt;&#xA;&lt;li&gt;Press &lt;code&gt;Apply&lt;/code&gt;, then &lt;code&gt;Connect&lt;/code&gt;, then &lt;code&gt;Play&lt;/code&gt; to start acquiring an Explore stream in OpenViBE via Lab Streaming Layer.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;calibrate-eog-data&#34;&gt;Calibrate EOG data&lt;/h3&gt;&#xA;&lt;ol start=&#34;12&#34;&gt;&#xA;&lt;li&gt;Open the OpenViBE &lt;code&gt;designer&lt;/code&gt; and go to &lt;code&gt;File&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;Open&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Choose the &lt;code&gt;recorder.xml&lt;/code&gt; file (extracted from the ZIP file you downloaded earlier) and run the scenario for at least 60 seconds. Blink and turn your eyes to different directions during the recording. A &lt;code&gt;GDF&lt;/code&gt; file will be created in the same folder as the contents of your ZIP file.&lt;/li&gt;&#xA;&lt;li&gt;Run the EOG-calibration scenario in OpenViBE &lt;code&gt;designer&lt;/code&gt;. Go to &lt;code&gt;File&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;Open&lt;/code&gt;, and then choose the &lt;code&gt;EOG-calibration.xml&lt;/code&gt; file extracted earlier. Double click on the &lt;code&gt;GDF&lt;/code&gt; file reader and select the file generated in the previous step as source.&lt;/li&gt;&#xA;&lt;li&gt;Run the EOG-calibration scenario in OpenViBE. Go to &lt;code&gt;File&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;Open&lt;/code&gt; and select &lt;code&gt;EOG-calibration.xml&lt;/code&gt;. Click on the keyboard simulator window and press &lt;code&gt;a&lt;/code&gt; to start the calibration. Run the scenario until &lt;code&gt;b-Matrix-EEG.cfg&lt;/code&gt; is generated (check the log). Usually 30-40 seconds are sufficient here. Do blink during the recording and press &lt;code&gt;u&lt;/code&gt; to end the data calibration.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;visualize-frequency-bands-with-eog-artifacts-removed&#34;&gt;Visualize frequency bands with EOG artifacts removed&lt;/h2&gt;&#xA;&lt;p&gt;Open the &lt;code&gt;Power_bands.xml&lt;/code&gt; scenario and run it. You will see EEG signals before and after EOG denoising. You can test the performance of the EOG artifact removal by blinking and checking both signals. In the other tabs, the power bands (Alpha, Beta and Theta) are plotted.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SSVEP Experiments</title>
      <link>http://localhost:1313/applications/ssvep/</link>
      <pubDate>Wed, 26 Jan 2022 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/ssvep/</guid>
      <description>&lt;p&gt;Steady state visually evoked potentials (SSVEPs) are commonly used in neuroscience and BCI research.&lt;/p&gt;&#xA;&lt;p&gt;SSVEPs are brain signals that occur in response to a visual stimulus flickering at a fixed frequency (typically 3-75 Hz). A rhythmic stimulus can entrain brain activity in the occipital lobe, which is commonly associated with the visual cortex.&lt;/p&gt;&#xA;&lt;p&gt;Here, we show you how to use Mentalab Explore with SSVEP-based classification tasks. The code is open-source and available &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy/tree/master/examples/ssvep_demo&#34;&gt;here&lt;/a&gt;. You can run it as is or use it to build your own SSVEP based applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EASI ECG</title>
      <link>http://localhost:1313/applications/easi-ecg/</link>
      <pubDate>Wed, 26 Jan 2022 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/easi-ecg/</guid>
      <description>&lt;figure style=&#34;float:right;text-align:center;&#34;&gt;&#xA;&lt;img class=&#34;mobile&#34; src=&#34;http://localhost:1313/img/Applications/ECG_Placement_of_Electrodes.png&#34; alt=&#34;12 lead configuration&#34; width=&#34;200&#34;/&gt;&#xA;&lt;figcaption&gt; The 12 lead electrode configuration. &lt;br&gt;Source: &lt;a href=&#34;https://www.cardiosecur.com/magazine/specialist-articles-on-the-heart/lead-systems-how-an-ecg-works&#34;&gt;CardioSecur&lt;/a&gt; &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;p&gt;Conventional 12-lead ECG uses 10 electrodes (6 electrodes on the chest and 4 electrodes on the limbs&amp;rsquo; extremities).&lt;/p&gt;&#xA;&lt;p&gt;Although it provides comprehensive information about electrical processes inside the heart, the number of electrodes and the long distance between them makes 12-lead ECG impractical and uncomfortable. This is especially true if the participant must remain still for a lengthy recording sessions.&lt;/p&gt;&#xA;&lt;p&gt;Summary of 12-lead ECG drawbacks:&lt;/p&gt;</description>
    </item>
    <item>
      <title>P300 Experiment</title>
      <link>http://localhost:1313/applications/p300/</link>
      <pubDate>Wed, 26 Jan 2022 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/p300/</guid>
      <description>&lt;p&gt;P300 signals are brain signals that occur in response to a visual stimulus. They tend to occur approximately 300 ms after the stimulus appears.&lt;/p&gt;&#xA;&lt;p&gt;Here, we show you how to use Mentalab Explore and an oddball experiment to elicit P300 signals. The code is open-source and available &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy/tree/master/examples/p300_demo&#34;&gt;here&lt;/a&gt;. You can run it as is or use it to build your own P300 based applications.&lt;/p&gt;&#xA;&lt;div class=&#34;video-container&#34;&gt;&#xA;&lt;iframe src=&#34;https://www.youtube-nocookie.com/embed/ncVuE96Chck&#34; title=&#34;SSVEP Experiment&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&lt;br&gt;&#xA;In this experiment, the oddball experiment has two visual stimuli: a blue rectangle and a red oval. The red oval is the so-called &#34;oddball&#34;. The participant must press `space` whenever the red stimulus appears.&#xA;&lt;br&gt;&lt;br&gt;&#xA;&lt;figure style=&#34;text-align:center;&#34;&gt;&#xA;&lt;img class=&#34;mobile&#34; src=&#34;http://localhost:1313/img/Applications/p300-stimuli.jpg&#34; alt=&#34;P300 visual stimuli&#34; width=&#34;90%&#34;/&gt;&#xA;&lt;figcaption&gt; A blue rectangle is the standard stimulus. The red oval is the &#34;oddball&#34;. When presented with the oddball, participants must press &lt;code&gt;space&lt;/code&gt;.&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;&#xA;&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;code&gt;&lt;a href=&#34;https://explorepy.readthedocs.io/&#34;&gt;explorepy&lt;/a&gt;&lt;/code&gt;. Choose Option 2 (Python) if you are installing &lt;code&gt;explorepy&lt;/code&gt; on a Windows system.&lt;/li&gt;&#xA;&lt;li&gt;Download the &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy&#34;&gt;&lt;code&gt;explorepy&lt;/code&gt; code&lt;/a&gt; directly from GitHub. To do this, click on the green &lt;code&gt;Code&lt;/code&gt; button, and &amp;ldquo;Download ZIP&amp;rdquo;. Remember where you extract the files, you will need this location later.&lt;/li&gt;&#xA;&lt;li&gt;Activate your Anaconda virtual environment: &lt;code&gt;conda activate myenv&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install the required packages: &lt;code&gt;pip install scikit-learn matplotlib psychopy mne&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the Conda terminal, navigate to the &lt;code&gt;p300_demo&lt;/code&gt; folder in the &lt;code&gt;examples&lt;/code&gt; directory of &lt;code&gt;explorepy&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;&#xA;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Setup the cap and electrodes. Place EEG electrodes in the desired locations (e.g. Cz, Pz, CP1, CP2, P3, P4, Oz, etc.) and the ground electrode on mastoid (or any other location far enough from other electrodes).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;p300-experiment&#34;&gt;P300 Experiment&lt;/h2&gt;&#xA;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Turn on the device, ensure it is advertising, and then run the following command in your terminal (replace #### with your device ID, e.g. Explore_1438):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sleep Recording Protocol</title>
      <link>http://localhost:1313/applications/sleep-recording-protocol/</link>
      <pubDate>Thu, 26 Jan 2023 10:25:19 +0000</pubDate>
      <guid>http://localhost:1313/applications/sleep-recording-protocol/</guid>
      <description>&lt;p&gt;This is a step-by-step guide describing how you can get the most out of your EEG sleep recording using Mentalab Explore.&lt;/p&gt;&#xA;&#xA;&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;&#xA;    Battery life is affected by sampling rate and Bluetooth connection. To maximize your recording time, we recommend offline recording at 250Hz.&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;setting-up-your-mentalab-system&#34;&gt;Setting up your Mentalab system&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Fully charge your amplifier then disconnect it from the USB port.&lt;/li&gt;&#xA;&lt;li&gt;Set up your participant&amp;rsquo;s cap and electrodes.&lt;/li&gt;&#xA;&lt;li&gt;Open &lt;a href=&#34;https://wiki.mentalab.com/explore-desktop-guide/&#34;&gt;Explore Desktop&lt;/a&gt; and connect your amplifier to your computer via Bluetooth.&lt;/li&gt;&#xA;&lt;li&gt;Check your system&amp;rsquo;s impedances using Explore Desktop. If you need to improve your impedances, please consult: our guide on &lt;a href=&#34;https://wiki.mentalab.com/user-guide/improving-impedances/&#34;&gt;improving impedances&lt;/a&gt;, our &lt;a href=&#34;https://wiki.mentalab.com/user-guide/eeg-pre-experiment-checklist/&#34;&gt;pre-experiment checklist&lt;/a&gt;, and our &lt;a href=&#34;https://wiki.mentalab.com/user-guide/dry-electrode-guide/&#34;&gt;dry electrode guide&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;End Explore Desktop&amp;rsquo;s impedance mode and go to the &lt;a href=&#34;https://wiki.mentalab.com/explore-desktop-guide/usage/#configuration&#34;&gt;Configuration page&lt;/a&gt;. Name your channels and apply the changes.&lt;/li&gt;&#xA;&lt;li&gt;Format your memory, and disconnect from Bluetooth. Your device should be blinking green. This indicates that it is recording in offline mode.&lt;/li&gt;&#xA;&lt;li&gt;Go to sleep! (And try to avoid moving the electrodes.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;exporting-your-data&#34;&gt;Exporting your data&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Ensure that your amplifier is turned off (no blinking of any kind).&lt;/li&gt;&#xA;&lt;li&gt;Disconnect the electrodes from your amplifier.&lt;/li&gt;&#xA;&lt;li&gt;Connect your amplifier to a computer using the provided USB cable.&lt;/li&gt;&#xA;&lt;li&gt;Export the largest &lt;code&gt;BIN&lt;/code&gt; file you can see on the internal memory.&lt;/li&gt;&#xA;&lt;li&gt;Convert the &lt;code&gt;BIN&lt;/code&gt; file to an appropriate file type using Explore Desktop&amp;rsquo;s File menu (top left corner).&lt;/li&gt;&#xA;&lt;li&gt;Finally, charge your amplifier and clean your cap and electrodes thoroughly. For more on cleaning, see our &lt;a href=&#34;https://wiki.mentalab.com/user-guide/cleaning-disinfection-storage/&#34;&gt;cleaning guide&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;For more information or support, do not hesitate to get in contact at: &lt;a href=&#34;mailto:support@mentalab.com&#34;&gt;support@mentalab.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sleep EEG Analysis</title>
      <link>http://localhost:1313/applications/sleep-analysis/</link>
      <pubDate>Thu, 22 Aug 2024 14:29:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/sleep-analysis/</guid>
      <description>&lt;p&gt;Modern sleep science is fundamentally informed by ExG biosensors recording brain activity (electroencephalography, EEG), eye-movement activity (electrooculography, EOG), and muscle activity (electromyography, EMG) to characterize the physiology of sleep.&lt;/p&gt;&#xA;&lt;p&gt;Here, we demonstrate how EEG data recorded during a full night&amp;rsquo;s sleep using Mentalab Explore can be analysed with the free and open source &lt;a href=&#34;https://raphaelvallat.com/yasa/build/html/index.html&#34;&gt;yasa toolbox&lt;/a&gt;. We will perform automatic sleep staging by applying the yasa classifier &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; to EEG data, inspect band power in the EEG signal, and detect slow waves and sleep spindles. The application example follows the excellent &lt;a href=&#34;https://github.com/raphaelvallat/yasa/tree/master/notebooks&#34;&gt;yasa notebooks&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ECG Analysis</title>
      <link>http://localhost:1313/applications/ecg-analysis/</link>
      <pubDate>Thu, 22 Aug 2024 14:29:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/ecg-analysis/</guid>
      <description>&lt;p&gt;Mentalab Explore is designed to record all kinds of ExG data. In this guide, we demonstrate how to use the &lt;a href=&#34;https://neuropsychology.github.io/NeuroKit/&#34;&gt;Neurokit2&lt;/a&gt; toolbox to analyse electrocardiography (ECG) data &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt;. See the list of neurokit &lt;a href=&#34;https://neuropsychology.github.io/NeuroKit/examples/index.html&#34;&gt;examples&lt;/a&gt; on which this tutorial is based for further resources.&lt;/p&gt;&#xA;&lt;h1 id=&#34;data-acquisition&#34;&gt;Data acquisition&lt;/h1&gt;&#xA;&lt;p&gt;We used two medical adhesive single-use electrodes to record the ECG. One electrode was placed on the left mastoid bone, whereas the second electrode was placed close to the heart on our volunteer&amp;rsquo;s chest. Using &lt;a href=&#34;https://wiki.mentalab.com/explore-desktop-guide/&#34;&gt;Explore Desktop&lt;/a&gt;, we recorded data for 5 minutes in a resting state, where the participant sat down. Data were saved in *.bdf format (European data format in 24 bit resolution).&lt;/p&gt;</description>
    </item>
    <item>
      <title>EEGSynth</title>
      <link>http://localhost:1313/applications/eegsynth/</link>
      <pubDate>Thu, 22 Aug 2024 14:29:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/eegsynth/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.eegsynth.org/&#34;&gt;EEGSynth&lt;/a&gt; is a software project aiming at connecting brain signals to electronic music instruments. EEGSynth is taking inspiration from &lt;a href=&#34;https://en.wikipedia.org/wiki/Modular_synthesizer&#34;&gt;modular synthesizers&lt;/a&gt;. In a modular synth, each module performs a specific function (generating sound, modulating sound, generating control voltages, etc.) and outputs from one module can be connected to the inputs of another module using patch cables. The entirety of the connected modules is referred to as a patch.&#xA;Similarly, EEGSynth modules can, among many other things, take in real-time EEG signals, process them and extract useful information such as bandpower, and use these to send instruction messages to electronic music instruments using the MIDI standard.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Real-time Spectrogram</title>
      <link>http://localhost:1313/applications/spectrogram/</link>
      <pubDate>Thu, 08 May 2025 12:54:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/spectrogram/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The spectrogram of a signal contains the magnitude of the frequencies of the signal over time, meaning it contains three dimensions: time, frequency and magnitude. This data can be plotted as an image that is essentially a heatmap of frequencies over time.&lt;/p&gt;&#xA;&lt;p&gt;This example application shows how this can be achieved with real-time updates using the Mentalab Explore device, explorepy and a few other dependencies.&lt;/p&gt;&#xA;&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Mentalab Explore&lt;/li&gt;&#xA;&lt;li&gt;A Python 3 installation, we strongly recommend using &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; to install Python and using version 3.12&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;setting-up-the-environment-and-dependencies&#34;&gt;Setting up the environment and dependencies&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set up an environment with Python version 3.12 with conda and activate it:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda create &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;n spectrogram python&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3.12&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda activate spectrogram&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Install liblsl, which is required by explorepy, via conda-forge:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;conda install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;c conda&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;forge liblsl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install the other dependencies via pip:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install explorepy glfw vispy scipy numpy&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&#xA;&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;&#xA;    glfw is used as a backend for vispy to display images. &lt;a href=&#34;https://vispy.org/installation.html&#34;&gt;Alternatively Qt, PySide6 and multiple other libraries equipped for drawing and displaying windows can be used&lt;/a&gt;.&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;running-the-example&#34;&gt;Running the example&lt;/h2&gt;&#xA;&lt;h3 id=&#34;adapting-the-script-to-your-needs&#34;&gt;Adapting the script to your needs&lt;/h3&gt;&#xA;&lt;p&gt;The code for this example (&lt;code&gt;real_time_spectrogram.py&lt;/code&gt;) can be found in the &lt;code&gt;examples&lt;/code&gt; folder of the GitHub repository for explorepy: &lt;a href=&#34;https://github.com/Mentalab-hub/explorepy/&#34;&gt;https://github.com/Mentalab-hub/explorepy/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>EEG to lamp brightness</title>
      <link>http://localhost:1313/applications/eeg-to-lamp/</link>
      <pubDate>Tue, 03 Jun 2025 12:54:00 +0100</pubDate>
      <guid>http://localhost:1313/applications/eeg-to-lamp/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;An EEG signal contains a multitude of frequencies which can give us an idea of what state the subject is in. For example, frequencies in the alpha range (8Hz - 13Hz) can indicate that the subject is relaxed and has their eyes closed. In this code example, the aforementioned phenomenon is creatively applied to control a lamp based on the subject&amp;rsquo;s relaxedness. If the subject relaxes and power in the alpha band increases, the LEDs increase in brightness and turn yellow. If the power in the alpha band decreases, the LEDs dim and turn turquoise.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
